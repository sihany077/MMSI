# MMSI-Bench
## Dataset
The MMSI-Bench dataset is available at:  
[https://huggingface.co/datasets/sihany/MMSI-Bench](https://huggingface.co/datasets/sihany/MMSI-Bench)

### Dataset Format

Each sample in the MMSI-Bench dataset is structured as follows:

- **id** (`str` or `int`): Unique identifier for the sample.
- **question** (`str`): The question or prompt related to the images.
- **images** (`List[bytes]`): A list of image contents in binary format (each image is stored as bytes).
- **answer** (`str`): The answer or response associated with the question and images.
- **question_type** (`str`):The category or type of the question.
- **thought** (`str`): The intermediate reasoning  generated by the annotators in response to the question.
  


## Environment Setup
```bash
conda create -n mmsi python=3.9 -y
conda activate mmsi
cd lmms-eval; pip install -e .
```
For open-source models, set up the environment according to the instructions at: https://github.com/EvolvingLMMs-Lab/lmms-eval


## Evaluation
### Evaluate OpenAI-Compatible Models
```bash
export OPENAI_COMPATIBLE_API_KEY=XXX
export OPENAI_COMPATIBLE_API_URL=XXX
lmms-eval --model openai_compatible --model_args model_version=gpt-4o-2024-08-06 --task msr_bench  --batch_size 1 --log_samples --log_samples_suffix gpt-4o-2024-08-06 --output_path ./logs
```
### Evaluate Open-Source Models
```bash
CUDA_VISIBLE_DEVICES=0,1,2 lmms-eval --model qwen2_5_vl --model_args=pretrained="path_to/Qwen2.5-VL-72B-Instruct",max_pixels=12835456,min_pixels=1003520,use_flash_attention_2=False \
    --tasks msr_bench \
    --log_samples --log_samples_suffix Qwen2.5-VL-72B \
    --batch_size 1 \
    --output_path ./logs
```
